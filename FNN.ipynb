{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  1. FeedForward Neural Network (FNN)\n",
    "\n",
    "\n",
    "- Data flows in one Direction \n",
    "-Input layer ->Hidden layer(if present) -> Output layer \n",
    "-Flow of information \n",
    "input -> weighted sum -> Activation function -> Output\n",
    "\n",
    "Mathematical representation of each neuron \n",
    "    y=f(âˆ‘(wâ‹…x)+b)\n",
    "    f= Activation Function \n",
    "\n",
    "\n",
    "\n",
    "Activation Functions: \n",
    "    Activation function introduces non - Linearity into network ,allowing the model to learn complex patterns \n",
    "\n",
    "\n",
    "    Common activation functions: \n",
    "        ReLu(Recified Linear Unit) : f(x) = max(0,x)\n",
    "        sigmoid :f(x)  = 1/1+e^-x  [for binary classification]\n",
    "        Tanh : f(x) = tanh(x) [scales output between -1 to 1 ]\n",
    "        Softmax :  Converts outputs into probabilities (for multi-class classification )\n",
    "\n",
    "\n",
    "\n",
    "Learning Process : \n",
    "    Fnn learn by adjusting weights through BackPropagation and Gradient Descent \n",
    "\n",
    "    1. Forward Pass : \n",
    "            Input is Passed through layers to produce an output \n",
    "    2.  Loss Calculation  : \n",
    "            Measures difference between predicted output and actual values using a loss function (e.g. Mean Squared Errors)\n",
    "    3.  Backward Pass : \n",
    "            Compute gradients of loss function w.r.t weights using the chain rule \n",
    "    4.  Weight Update : \n",
    "            Update weights using an optimization algorithm \n",
    "\n",
    "\n",
    "\n",
    "Types  of FNN : \n",
    "    -Single Layer Perceptron(SLP):  No hidden layer , suitable for linear problems \n",
    "    -Multiple Layer Perceptron(MLP) : One or more hidden layer  \n",
    "\n",
    "Advantage : \n",
    "    -Simple to implement \n",
    "    -effective for tabular data \n",
    "    -works well for smaller datasets \n",
    "\n",
    "Disadvantage : \n",
    "    -Cannot handle sequential Data \n",
    "    -Prone to overfitting on large, complex datasets \n",
    "    -Computationally expensive for deep architectures \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m \n",
    "import random \n",
    "\n",
    "\n",
    "\n",
    "#activation function (Sigmoid) \n",
    "def sigmoid(x): \n",
    "    return 1/(1+m.exp(-x))\n",
    "\n",
    "\n",
    "def derivative_sigmoid(x): \n",
    "    return (x)*(1-x)\n",
    "\n",
    "\n",
    "\n",
    "def init_weight(input_size, hidden_size , output_size ) : \n",
    "\n",
    "    w1 = [[random.uniform(-1,1) for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "    w2 = [random.uniform(-1,1) for _ in range(hidden_size)]\n",
    "\n",
    "\n",
    "    return w1,w2 \n",
    "\n",
    "def forward_propagation(inputs,w1,w2): \n",
    "\n",
    "    hidden_input = [sum(i * w1[k][j] for k, i in enumerate(inputs)) for j in range(len(w1[0]))]\n",
    "    hidden_output = [sigmoid(x) for x in hidden_input] \n",
    "    \n",
    "    output = sum(h*w for h , w in zip(hidden_output , w2 ))\n",
    "    output = sigmoid(output)\n",
    "\n",
    "    return hidden_input , output \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation( inputs ,target ,w1,w2 , hidden_output , output , learning_rate ) : \n",
    "    output_error = target - output \n",
    "    output_delta = output_error * derivative_sigmoid(output)\n",
    "\n",
    "    hidden_error = [output_delta* w for w in w2 ] \n",
    "\n",
    "    hidden_delta = [he * sigmoid(h) for he , h in zip(hidden_error , hidden_output)]\n",
    "\n",
    "    for i in range(len(w2)) : \n",
    "        w2[i] += learning_rate * output_delta * hidden_output[i]\n",
    "    \n",
    "    for i in range(len(w1)) : \n",
    "        for j in range(len(w1[i])) :\n",
    "            w1[i][j] += learning_rate *hidden_delta[j] * inputs[i] \n",
    "\n",
    "    return w1 , w2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,y , input_size , hidden_size , output_size , learning_rate , epochs ):\n",
    "    w1, w2 = init_weight(input_size, hidden_size , output_size)\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        total_error  = 0 \n",
    "\n",
    "\n",
    "        for i in range(len(X)) : \n",
    "            inputs = X[i]\n",
    "            targets = y[i]\n",
    "\n",
    "            # forward pass \n",
    "            hidden_outputs , outputs =forward_propagation(inputs, w1, w2) \n",
    "\n",
    "            # compute errors  \n",
    "            total_error +=  (targets- outputs )**2\n",
    "\n",
    "            # back propagations \n",
    "            w1, w2 = back_propagation(inputs , targets , w1,w2, hidden_outputs, outputs , learning_rate)\n",
    "\n",
    "            if epoch%100 == 0 : \n",
    "                print(f'Epoch {epoch} , Error : {total_error}')\n",
    "        \n",
    "    \n",
    "    return w1 ,w2 \n",
    "\n",
    "\n",
    "def predict(inputs , w1,w2) : \n",
    "    _ , output = forward_propagation(inputs , w1 , w2 ) \n",
    "    return 1 if output>=0.5 else 0 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# Parameters\n",
    "input_size = 2\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , Error : 0.4761794848187276\n",
      "Epoch 0 , Error : 0.6127786078206449\n",
      "Epoch 0 , Error : 0.7265844242152272\n",
      "Epoch 0 , Error : 1.0919874103370628\n",
      "Epoch 100 , Error : 0.4890914146609268\n",
      "Epoch 100 , Error : 0.6226667925202121\n",
      "Epoch 100 , Error : 0.7337417021203894\n",
      "Epoch 100 , Error : 1.0971261413428792\n",
      "Epoch 200 , Error : 0.5024033935257515\n",
      "Epoch 200 , Error : 0.6330177213185669\n",
      "Epoch 200 , Error : 0.7413937818492734\n",
      "Epoch 200 , Error : 1.1023746687408924\n",
      "Epoch 300 , Error : 0.5160453026822661\n",
      "Epoch 300 , Error : 0.6438371655741124\n",
      "Epoch 300 , Error : 0.7496222695825709\n",
      "Epoch 300 , Error : 1.1075950784397137\n",
      "Epoch 400 , Error : 0.5299230256217419\n",
      "Epoch 400 , Error : 0.6551122613640273\n",
      "Epoch 400 , Error : 0.758498441102265\n",
      "Epoch 400 , Error : 1.112623377930771\n",
      "Epoch 500 , Error : 0.5439150952507011\n",
      "Epoch 500 , Error : 0.6668078429785549\n",
      "Epoch 500 , Error : 0.7680806755377098\n",
      "Epoch 500 , Error : 1.117273228650457\n",
      "Epoch 600 , Error : 0.5578702819544685\n",
      "Epoch 600 , Error : 0.6788625868417664\n",
      "Epoch 600 , Error : 0.7784112318249088\n",
      "Epoch 600 , Error : 1.1213460276346934\n",
      "Epoch 700 , Error : 0.5716070426182784\n",
      "Epoch 700 , Error : 0.6911852188524814\n",
      "Epoch 700 , Error : 0.7895121150618046\n",
      "Epoch 700 , Error : 1.124649685958226\n",
      "Epoch 800 , Error : 0.5849161641151558\n",
      "Epoch 800 , Error : 0.7036511835694714\n",
      "Epoch 800 , Error : 0.8013795203331184\n",
      "Epoch 800 , Error : 1.1270275664979204\n",
      "Epoch 900 , Error : 0.597568296628494\n",
      "Epoch 900 , Error : 0.7161004448959829\n",
      "Epoch 900 , Error : 0.8139761299664068\n",
      "Epoch 900 , Error : 1.1283961039414876\n",
      "Epoch 1000 , Error : 0.609328055281935\n",
      "Epoch 1000 , Error : 0.7283375599551256\n",
      "Epoch 1000 , Error : 0.8272207129312908\n",
      "Epoch 1000 , Error : 1.1287838496226485\n",
      "Epoch 1100 , Error : 0.619975416447676\n",
      "Epoch 1100 , Error : 0.7401358425913448\n",
      "Epoch 1100 , Error : 0.8409756215161326\n",
      "Epoch 1100 , Error : 1.1283571134522814\n",
      "Epoch 1200 , Error : 0.6293327237831672\n",
      "Epoch 1200 , Error : 0.7512480369160723\n",
      "Epoch 1200 , Error : 0.8550354278235159\n",
      "Epoch 1200 , Error : 1.127412980063197\n",
      "Epoch 1300 , Error : 0.6372919952732249\n",
      "Epoch 1300 , Error : 0.7614256776967943\n",
      "Epoch 1300 , Error : 0.8691235902905068\n",
      "Epoch 1300 , Error : 1.1263274021034415\n",
      "Epoch 1400 , Error : 0.6438341935280033\n",
      "Epoch 1400 , Error : 0.770447125533709\n",
      "Epoch 1400 , Error : 0.8829060427207952\n",
      "Epoch 1400 , Error : 1.1254678888323464\n",
      "Epoch 1500 , Error : 0.6490325442542957\n",
      "Epoch 1500 , Error : 0.778149849442578\n",
      "Epoch 1500 , Error : 0.896026956632118\n",
      "Epoch 1500 , Error : 1.1251054899459287\n",
      "Epoch 1600 , Error : 0.6530372011497618\n",
      "Epoch 1600 , Error : 0.7844579248151207\n",
      "Epoch 1600 , Error : 0.9081615472707112\n",
      "Epoch 1600 , Error : 1.1253665706127807\n",
      "Epoch 1700 , Error : 0.6560460530115376\n",
      "Epoch 1700 , Error : 0.7893947751322705\n",
      "Epoch 1700 , Error : 0.9190694807752038\n",
      "Epoch 1700 , Error : 1.1262415904118528\n",
      "Epoch 1800 , Error : 0.6582714112287089\n",
      "Epoch 1800 , Error : 0.7930760017000195\n",
      "Epoch 1800 , Error : 0.9286294629390135\n",
      "Epoch 1800 , Error : 1.1276345571957047\n",
      "Epoch 1900 , Error : 0.6599119496843432\n",
      "Epoch 1900 , Error : 0.7956851298708087\n",
      "Epoch 1900 , Error : 0.936844168161124\n",
      "Epoch 1900 , Error : 1.1294205478473784\n",
      "Epoch 2000 , Error : 0.6611350108876672\n",
      "Epoch 2000 , Error : 0.7974410508220895\n",
      "Epoch 2000 , Error : 0.94381826431706\n",
      "Epoch 2000 , Error : 1.1314866107732695\n",
      "Epoch 2100 , Error : 0.6620696288689998\n",
      "Epoch 2100 , Error : 0.7985668249757972\n",
      "Epoch 2100 , Error : 0.9497216317076465\n",
      "Epoch 2100 , Error : 1.1337492697929525\n",
      "Epoch 2200 , Error : 0.6628076512808283\n",
      "Epoch 2200 , Error : 0.7992663236726569\n",
      "Epoch 2200 , Error : 0.9547511618397211\n",
      "Epoch 2200 , Error : 1.1361546118235104\n",
      "Epoch 2300 , Error : 0.6634095003970879\n",
      "Epoch 2300 , Error : 0.7997108429560714\n",
      "Epoch 2300 , Error : 0.9591001924257374\n",
      "Epoch 2300 , Error : 1.1386703429081373\n",
      "Epoch 2400 , Error : 0.6639116265163065\n",
      "Epoch 2400 , Error : 0.8000345610053555\n",
      "Epoch 2400 , Error : 0.9629390191123824\n",
      "Epoch 2400 , Error : 1.1412768002028073\n",
      "Epoch 2500 , Error : 0.664333710785148\n",
      "Epoch 2500 , Error : 0.8003362016732225\n",
      "Epoch 2500 , Error : 0.966405801221194\n",
      "Epoch 2500 , Error : 1.1439602164585307\n",
      "Epoch 2600 , Error : 0.6646846281408325\n",
      "Epoch 2600 , Error : 0.8006841371243474\n",
      "Epoch 2600 , Error : 0.9696052083916827\n",
      "Epoch 2600 , Error : 1.1467088768547562\n",
      "Epoch 2700 , Error : 0.6649668711101236\n",
      "Epoch 2700 , Error : 0.801122747566771\n",
      "Epoch 2700 , Error : 0.9726118118834332\n",
      "Epoch 2700 , Error : 1.1495115636676212\n",
      "Epoch 2800 , Error : 0.66517954157354\n",
      "Epoch 2800 , Error : 0.8016786278437109\n",
      "Epoch 2800 , Error : 0.9754757297380199\n",
      "Epoch 2800 , Error : 1.1523574012430824\n",
      "Epoch 2900 , Error : 0.6653202014272189\n",
      "Epoch 2900 , Error : 0.8023659027744241\n",
      "Epoch 2900 , Error : 0.978228801943479\n",
      "Epoch 2900 , Error : 1.1552363684797755\n",
      "Epoch 3000 , Error : 0.665385912511273\n",
      "Epoch 3000 , Error : 0.8031903876863171\n",
      "Epoch 3000 , Error : 0.9808902820866429\n",
      "Epoch 3000 , Error : 1.1581400047050747\n",
      "Epoch 3100 , Error : 0.6653737581082599\n",
      "Epoch 3100 , Error : 0.8041526140391214\n",
      "Epoch 3100 , Error : 0.9834715657782871\n",
      "Epoch 3100 , Error : 1.1610620514465368\n",
      "Epoch 3200 , Error : 0.6652810698331969\n",
      "Epoch 3200 , Error : 0.8052498792721433\n",
      "Epoch 3200 , Error : 0.9859798225202713\n",
      "Epoch 3200 , Error : 1.163998913100073\n",
      "Epoch 3300 , Error : 0.6651055129577808\n",
      "Epoch 3300 , Error : 0.8064775266069875\n",
      "Epoch 3300 , Error : 0.9884205916744081\n",
      "Epoch 3300 , Error : 1.1669498966140637\n",
      "Epoch 3400 , Error : 0.6648451237429442\n",
      "Epoch 3400 , Error : 0.807829656037174\n",
      "Epoch 3400 , Error : 0.9907994908895034\n",
      "Epoch 3400 , Error : 1.169917227289458\n",
      "Epoch 3500 , Error : 0.664498348369475\n",
      "Epoch 3500 , Error : 0.8092994403712024\n",
      "Epoch 3500 , Error : 0.9931232082026511\n",
      "Epoch 3500 , Error : 1.1729058540033053\n",
      "Epoch 3600 , Error : 0.6640641035969546\n",
      "Epoch 3600 , Error : 0.8108791867098717\n",
      "Epoch 3600 , Error : 0.995399938516655\n",
      "Epoch 3600 , Error : 1.1759230650320196\n",
      "Epoch 3700 , Error : 0.6635418612929016\n",
      "Epoch 3700 , Error : 0.8125602525640014\n",
      "Epoch 3700 , Error : 0.9976394022695231\n",
      "Epoch 3700 , Error : 1.1789779417887172\n",
      "Epoch 3800 , Error : 0.6629317491185764\n",
      "Epoch 3800 , Error : 0.8143329000624534\n",
      "Epoch 3800 , Error : 0.9998525609284241\n",
      "Epoch 3800 , Error : 1.1820806848081946\n",
      "Epoch 3900 , Error : 0.6622346551706443\n",
      "Epoch 3900 , Error : 0.8161861514432359\n",
      "Epoch 3900 , Error : 1.0020511257720006\n",
      "Epoch 3900 , Error : 1.1852418543057435\n",
      "Epoch 4000 , Error : 0.6614523233006099\n",
      "Epoch 4000 , Error : 0.8181076929147032\n",
      "Epoch 4000 , Error : 1.0042469440099848\n",
      "Epoch 4000 , Error : 1.1884715752223451\n",
      "Epoch 4100 , Error : 0.6605874269140007\n",
      "Epoch 4100 , Error : 0.8200838602889529\n",
      "Epoch 4100 , Error : 1.0064513377456024\n",
      "Epoch 4100 , Error : 1.1917787618085633\n",
      "Epoch 4200 , Error : 0.6596436115120039\n",
      "Epoch 4200 , Error : 0.8220997270602344\n",
      "Epoch 4200 , Error : 1.0086744637328569\n",
      "Epoch 4200 , Error : 1.1951704175022422\n",
      "Epoch 4300 , Error : 0.658625499552931\n",
      "Epoch 4300 , Error : 0.8241393029903197\n",
      "Epoch 4300 , Error : 1.01092475267936\n",
      "Epoch 4300 , Error : 1.1986510608040384\n",
      "Epoch 4400 , Error : 0.6575386549430898\n",
      "Epoch 4400 , Error : 0.8261858386984338\n",
      "Epoch 4400 , Error : 1.0132084744240346\n",
      "Epoch 4400 , Error : 1.2022223168570365\n",
      "Epoch 4500 , Error : 0.6563895081969016\n",
      "Epoch 4500 , Error : 0.8282222198695137\n",
      "Epoch 4500 , Error : 1.0155294595425144\n",
      "Epoch 4500 , Error : 1.2058826985907052\n",
      "Epoch 4600 , Error : 0.6551852466327511\n",
      "Epoch 4600 , Error : 0.8302314246013707\n",
      "Epoch 4600 , Error : 1.0178889900239485\n",
      "Epoch 4600 , Error : 1.2096275828037468\n",
      "Epoch 4700 , Error : 0.6539336765546433\n",
      "Epoch 4700 , Error : 0.8321970103449786\n",
      "Epoch 4700 , Error : 1.020285853753668\n",
      "Epoch 4700 , Error : 1.2134493682573033\n",
      "Epoch 4800 , Error : 0.6526430659875347\n",
      "Epoch 4800 , Error : 0.8341035938289559\n",
      "Epoch 4800 , Error : 1.0227165420082958\n",
      "Epoch 4800 , Error : 1.2173377874935438\n",
      "Epoch 4900 , Error : 0.6513219771159305\n",
      "Epoch 4900 , Error : 0.8359372886988676\n",
      "Epoch 4900 , Error : 1.0251755579878905\n",
      "Epoch 4900 , Error : 1.2212803337464115\n"
     ]
    }
   ],
   "source": [
    "w1,w2 = train(X , y , input_size , hidden_size ,output_size, learning_rate , epochs )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0, 0] -> output : 1 \n",
      "Input:[0, 1] -> output : 1 \n",
      "Input:[1, 0] -> output : 1 \n",
      "Input:[1, 1] -> output : 0 \n"
     ]
    }
   ],
   "source": [
    "for sample in X : \n",
    "    print(f'Input:{sample} -> outaput : {predict(sample,w1,w2)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use relu() now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return max(0,x)\n",
    "\n",
    "def relu_derivative(x) : \n",
    "    return 1 if x>0 else 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs,w1,w2): \n",
    "\n",
    "    hidden_input = [sum(i * w1[k][j] for k, i in enumerate(inputs)) for j in range(len(w1[0]))]\n",
    "    hidden_output = [relu(x) for x in hidden_input] \n",
    "    \n",
    "    output = sum(h*w for h , w in zip(hidden_output , w2 ))\n",
    "    output = relu(output)\n",
    "\n",
    "    return hidden_input , output \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation( inputs ,target ,w1,w2 , hidden_output , output , learning_rate ) : \n",
    "    output_error = target - output \n",
    "    output_delta = output_error * relu_derivative(output)\n",
    "\n",
    "    hidden_error = [output_delta* w for w in w2 ] \n",
    "\n",
    "    hidden_delta = [he * relu(h) for he , h in zip(hidden_error , hidden_output)]\n",
    "\n",
    "    for i in range(len(w2)) : \n",
    "        w2[i] += learning_rate * output_delta * hidden_output[i]\n",
    "\n",
    "    for i in range(len(w1)) : \n",
    "        for j in range(len(w1[i])) :\n",
    "            w1[i][j] += learning_rate *hidden_delta[j] * inputs[i] \n",
    "\n",
    "    return w1 , w2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , Error : 0\n",
      "Epoch 0 , Error : 0.07326551308354153\n",
      "Epoch 0 , Error : 0.3424667273043077\n",
      "Epoch 0 , Error : 1.084184437873068\n",
      "Epoch 100 , Error : 0\n",
      "Epoch 100 , Error : 0.1866463988532473\n",
      "Epoch 100 , Error : 0.2942920063310256\n",
      "Epoch 100 , Error : 0.563165021402223\n",
      "Epoch 200 , Error : 0\n",
      "Epoch 200 , Error : 0.14135783650127073\n",
      "Epoch 200 , Error : 0.2168838945298513\n",
      "Epoch 200 , Error : 0.45171609828483955\n",
      "Epoch 300 , Error : 0\n",
      "Epoch 300 , Error : 0.099250149903199\n",
      "Epoch 300 , Error : 0.1676589894967494\n",
      "Epoch 300 , Error : 0.37454785109178257\n",
      "Epoch 400 , Error : 0\n",
      "Epoch 400 , Error : 0.07614544291045781\n",
      "Epoch 400 , Error : 0.13942445610640197\n",
      "Epoch 400 , Error : 0.3251246636077426\n",
      "Epoch 500 , Error : 0\n",
      "Epoch 500 , Error : 0.06562194553907486\n",
      "Epoch 500 , Error : 0.12586649731483607\n",
      "Epoch 500 , Error : 0.2960701908475333\n",
      "Epoch 600 , Error : 0\n",
      "Epoch 600 , Error : 0.06242367775736983\n",
      "Epoch 600 , Error : 0.12253336319398157\n",
      "Epoch 600 , Error : 0.2817563730389375\n",
      "Epoch 700 , Error : 0\n",
      "Epoch 700 , Error : 0.0638791960522018\n",
      "Epoch 700 , Error : 0.12662919917023266\n",
      "Epoch 700 , Error : 0.27808496372495195\n",
      "Epoch 800 , Error : 0\n",
      "Epoch 800 , Error : 0.0684556486105795\n",
      "Epoch 800 , Error : 0.13603588858799515\n",
      "Epoch 800 , Error : 0.28169120782814167\n",
      "Epoch 900 , Error : 0\n",
      "Epoch 900 , Error : 0.0748050214133066\n",
      "Epoch 900 , Error : 0.14843024292287985\n",
      "Epoch 900 , Error : 0.289157848786327\n",
      "Epoch 1000 , Error : 0\n",
      "Epoch 1000 , Error : 0.08134512756523553\n",
      "Epoch 1000 , Error : 0.1608368936711665\n",
      "Epoch 1000 , Error : 0.2966242696328272\n",
      "Epoch 1100 , Error : 0\n",
      "Epoch 1100 , Error : 0.08634287498782084\n",
      "Epoch 1100 , Error : 0.16993562379533467\n",
      "Epoch 1100 , Error : 0.30008953578298736\n",
      "Epoch 1200 , Error : 0\n",
      "Epoch 1200 , Error : 0.0883629230008323\n",
      "Epoch 1200 , Error : 0.1729945109637837\n",
      "Epoch 1200 , Error : 0.2962940894777769\n",
      "Epoch 1300 , Error : 0\n",
      "Epoch 1300 , Error : 0.08669161947466543\n",
      "Epoch 1300 , Error : 0.1687201976627512\n",
      "Epoch 1300 , Error : 0.28357490840819566\n",
      "Epoch 1400 , Error : 0\n",
      "Epoch 1400 , Error : 0.08144696112943518\n",
      "Epoch 1400 , Error : 0.15748055191661378\n",
      "Epoch 1400 , Error : 0.2622027122087869\n",
      "Epoch 1500 , Error : 0\n",
      "Epoch 1500 , Error : 0.07343343590162843\n",
      "Epoch 1500 , Error : 0.14100709674700485\n",
      "Epoch 1500 , Error : 0.23420896144295453\n",
      "Epoch 1600 , Error : 0\n",
      "Epoch 1600 , Error : 0.0638841943100387\n",
      "Epoch 1600 , Error : 0.12184541521293119\n",
      "Epoch 1600 , Error : 0.20283133588939412\n",
      "Epoch 1700 , Error : 0\n",
      "Epoch 1700 , Error : 0.0546006498242529\n",
      "Epoch 1700 , Error : 0.10347589165972579\n",
      "Epoch 1700 , Error : 0.17858520922368837\n",
      "Epoch 1800 , Error : 0\n",
      "Epoch 1800 , Error : 0.07382520785498344\n",
      "Epoch 1800 , Error : 0.13991387342372677\n",
      "Epoch 1800 , Error : 0.2506270999008926\n",
      "Epoch 1900 , Error : 0\n",
      "Epoch 1900 , Error : 0.0987781023567467\n",
      "Epoch 1900 , Error : 0.18809885708999424\n",
      "Epoch 1900 , Error : 0.3256776026744017\n",
      "Epoch 2000 , Error : 0\n",
      "Epoch 2000 , Error : 0.12093311076815924\n",
      "Epoch 2000 , Error : 0.2308556571544083\n",
      "Epoch 2000 , Error : 0.39088562920800696\n",
      "Epoch 2100 , Error : 0\n",
      "Epoch 2100 , Error : 0.14010419035232877\n",
      "Epoch 2100 , Error : 0.26804388374404914\n",
      "Epoch 2100 , Error : 0.44712981637391636\n",
      "Epoch 2200 , Error : 0\n",
      "Epoch 2200 , Error : 0.15841990115529897\n",
      "Epoch 2200 , Error : 0.3040945447656438\n",
      "Epoch 2200 , Error : 0.5011234244809295\n",
      "Epoch 2300 , Error : 0\n",
      "Epoch 2300 , Error : 0.17912907560745492\n",
      "Epoch 2300 , Error : 0.34552407173768596\n",
      "Epoch 2300 , Error : 0.562639701388308\n",
      "Epoch 2400 , Error : 0\n",
      "Epoch 2400 , Error : 0.2062749434967891\n",
      "Epoch 2400 , Error : 0.4002653725629608\n",
      "Epoch 2400 , Error : 0.6439992608614801\n",
      "Epoch 2500 , Error : 0\n",
      "Epoch 2500 , Error : 0.24546493485045687\n",
      "Epoch 2500 , Error : 0.47922862694766566\n",
      "Epoch 2500 , Error : 0.7626387875023367\n",
      "Epoch 2600 , Error : 0\n",
      "Epoch 2600 , Error : 0.24925238804780547\n",
      "Epoch 2600 , Error : 0.490702238322278\n",
      "Epoch 2600 , Error : 0.756619127590836\n",
      "Epoch 2700 , Error : 0\n",
      "Epoch 2700 , Error : 0.24066967667243053\n",
      "Epoch 2700 , Error : 0.4762181709595853\n",
      "Epoch 2700 , Error : 0.7279035380058085\n",
      "Epoch 2800 , Error : 0\n",
      "Epoch 2800 , Error : 0.23300964932230514\n",
      "Epoch 2800 , Error : 0.46262096492596044\n",
      "Epoch 2800 , Error : 0.7032154310141451\n",
      "Epoch 2900 , Error : 0\n",
      "Epoch 2900 , Error : 0.2267196313279023\n",
      "Epoch 2900 , Error : 0.45113875470067755\n",
      "Epoch 2900 , Error : 0.6833109187427382\n",
      "Epoch 3000 , Error : 0\n",
      "Epoch 3000 , Error : 0.2217497180077185\n",
      "Epoch 3000 , Error : 0.4419104532214895\n",
      "Epoch 3000 , Error : 0.6677320496080986\n",
      "Epoch 3100 , Error : 0\n",
      "Epoch 3100 , Error : 0.2179080966624724\n",
      "Epoch 3100 , Error : 0.43469960365277516\n",
      "Epoch 3100 , Error : 0.6557454826246081\n",
      "Epoch 3200 , Error : 0\n",
      "Epoch 3200 , Error : 0.21498916113388808\n",
      "Epoch 3200 , Error : 0.42918306429254716\n",
      "Epoch 3200 , Error : 0.6466517675354541\n",
      "Epoch 3300 , Error : 0\n",
      "Epoch 3300 , Error : 0.21281255238783917\n",
      "Epoch 3300 , Error : 0.425053304560441\n",
      "Epoch 3300 , Error : 0.6398646144687995\n",
      "Epoch 3400 , Error : 0\n",
      "Epoch 3400 , Error : 0.2112303897462563\n",
      "Epoch 3400 , Error : 0.4220474045378707\n",
      "Epoch 3400 , Error : 0.6349143464285967\n",
      "Epoch 3500 , Error : 0\n",
      "Epoch 3500 , Error : 0.21012443652578397\n",
      "Epoch 3500 , Error : 0.41994969708087765\n",
      "Epoch 3500 , Error : 0.6314304790270432\n",
      "Epoch 3600 , Error : 0\n",
      "Epoch 3600 , Error : 0.2094009025053119\n",
      "Epoch 3600 , Error : 0.4185861228652864\n",
      "Epoch 3600 , Error : 0.6291216147908858\n",
      "Epoch 3700 , Error : 0\n",
      "Epoch 3700 , Error : 0.2089854003506783\n",
      "Epoch 3700 , Error : 0.4178169049797232\n",
      "Epoch 3700 , Error : 0.6277579325299224\n",
      "Epoch 3800 , Error : 0\n",
      "Epoch 3800 , Error : 0.20881872928046882\n",
      "Epoch 3800 , Error : 0.4175297596854429\n",
      "Epoch 3800 , Error : 0.6271572296144534\n",
      "Epoch 3900 , Error : 0\n",
      "Epoch 3900 , Error : 0.2088535391944385\n",
      "Epoch 3900 , Error : 0.4176342271741877\n",
      "Epoch 3900 , Error : 0.6271741649405475\n",
      "Epoch 4000 , Error : 0\n",
      "Epoch 4000 , Error : 0.20905174785340402\n",
      "Epoch 4000 , Error : 0.4180571325510716\n",
      "Epoch 4000 , Error : 0.6276920646032368\n",
      "Epoch 4100 , Error : 0\n",
      "Epoch 4100 , Error : 0.20938255468363756\n",
      "Epoch 4100 , Error : 0.4187390142803591\n",
      "Epoch 4100 , Error : 0.628616688412364\n",
      "Epoch 4200 , Error : 0\n",
      "Epoch 4200 , Error : 0.2098209127479351\n",
      "Epoch 4200 , Error : 0.4196313301128469\n",
      "Epoch 4200 , Error : 0.6298714706217399\n",
      "Epoch 4300 , Error : 0\n",
      "Epoch 4300 , Error : 0.2103463484354576\n",
      "Epoch 4300 , Error : 0.42069427112887126\n",
      "Epoch 4300 , Error : 0.6313938638966554\n",
      "Epoch 4400 , Error : 0\n",
      "Epoch 4400 , Error : 0.21094204446435705\n",
      "Epoch 4400 , Error : 0.4218950463258586\n",
      "Epoch 4400 , Error : 0.633132510537136\n",
      "Epoch 4500 , Error : 0\n",
      "Epoch 4500 , Error : 0.21159412284516285\n",
      "Epoch 4500 , Error : 0.42320653039906797\n",
      "Epoch 4500 , Error : 0.6350450374241695\n",
      "Epoch 4600 , Error : 0\n",
      "Epoch 4600 , Error : 0.21229108053591803\n",
      "Epoch 4600 , Error : 0.4246061924119554\n",
      "Epoch 4600 , Error : 0.6370963246759187\n",
      "Epoch 4700 , Error : 0\n",
      "Epoch 4700 , Error : 0.21302334249921873\n",
      "Epoch 4700 , Error : 0.4260752426596921\n",
      "Epoch 4700 , Error : 0.6392571370150291\n",
      "Epoch 4800 , Error : 0\n",
      "Epoch 4800 , Error : 0.21378290570151398\n",
      "Epoch 4800 , Error : 0.427597949977348\n",
      "Epoch 4800 , Error : 0.6415030351824824\n",
      "Epoch 4900 , Error : 0\n",
      "Epoch 4900 , Error : 0.21456305408808604\n",
      "Epoch 4900 , Error : 0.4291610930080598\n",
      "Epoch 4900 , Error : 0.6438135053451576\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = train(X,y ,input_size , hidden_size , output_size , learning_rate ,epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0, 0] -> outaput : 0 \n",
      "Input:[0, 1] -> outaput : 1 \n",
      "Input:[1, 0] -> outaput : 1 \n",
      "Input:[1, 1] -> outaput : 0 \n"
     ]
    }
   ],
   "source": [
    "for sample in X : \n",
    "    print(f'Input:{sample} -> outaput : {predict(sample,w1,w2)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets  use tan h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x): \n",
    "    return (m.exp(x) + m.exp(-x) )/(m.exp(x) + m.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(x) : \n",
    "    return 1 - tanh(x) ** 2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs,w1,w2): \n",
    "\n",
    "    hidden_input = [sum(i * w1[k][j] for k, i in enumerate(inputs)) for j in range(len(w1[0]))]\n",
    "    hidden_output = [tanh(x) for x in hidden_input] \n",
    "    \n",
    "    output = sum(h*w for h , w in zip(hidden_output , w2 ))\n",
    "    output = tanh(output)\n",
    "\n",
    "    return hidden_input , output \n",
    "\n",
    "\n",
    "def back_propagation( inputs ,target ,w1,w2 , hidden_output , output , learning_rate ) : \n",
    "    output_error = target - output \n",
    "    output_delta = output_error * tanh_derivative(output)\n",
    "\n",
    "    hidden_error = [output_delta* w for w in w2 ] \n",
    "\n",
    "    hidden_delta = [he * tanh(h) for he , h in zip(hidden_error , hidden_output)]\n",
    "\n",
    "    for i in range(len(w2)) : \n",
    "        w2[i] += learning_rate * output_delta * hidden_output[i]\n",
    "\n",
    "    for i in range(len(w1)) : \n",
    "        for j in range(len(w1[i])) :\n",
    "            w1[i][j] += learning_rate *hidden_delta[j] * inputs[i] \n",
    "\n",
    "    return w1 , w2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , Error : 1.0\n",
      "Epoch 0 , Error : 1.0\n",
      "Epoch 0 , Error : 1.0\n",
      "Epoch 0 , Error : 2.0\n",
      "Epoch 100 , Error : 1.0\n",
      "Epoch 100 , Error : 1.0\n",
      "Epoch 100 , Error : 1.0\n",
      "Epoch 100 , Error : 2.0\n",
      "Epoch 200 , Error : 1.0\n",
      "Epoch 200 , Error : 1.0\n",
      "Epoch 200 , Error : 1.0\n",
      "Epoch 200 , Error : 2.0\n",
      "Epoch 300 , Error : 1.0\n",
      "Epoch 300 , Error : 1.0\n",
      "Epoch 300 , Error : 1.0\n",
      "Epoch 300 , Error : 2.0\n",
      "Epoch 400 , Error : 1.0\n",
      "Epoch 400 , Error : 1.0\n",
      "Epoch 400 , Error : 1.0\n",
      "Epoch 400 , Error : 2.0\n",
      "Epoch 500 , Error : 1.0\n",
      "Epoch 500 , Error : 1.0\n",
      "Epoch 500 , Error : 1.0\n",
      "Epoch 500 , Error : 2.0\n",
      "Epoch 600 , Error : 1.0\n",
      "Epoch 600 , Error : 1.0\n",
      "Epoch 600 , Error : 1.0\n",
      "Epoch 600 , Error : 2.0\n",
      "Epoch 700 , Error : 1.0\n",
      "Epoch 700 , Error : 1.0\n",
      "Epoch 700 , Error : 1.0\n",
      "Epoch 700 , Error : 2.0\n",
      "Epoch 800 , Error : 1.0\n",
      "Epoch 800 , Error : 1.0\n",
      "Epoch 800 , Error : 1.0\n",
      "Epoch 800 , Error : 2.0\n",
      "Epoch 900 , Error : 1.0\n",
      "Epoch 900 , Error : 1.0\n",
      "Epoch 900 , Error : 1.0\n",
      "Epoch 900 , Error : 2.0\n",
      "Epoch 1000 , Error : 1.0\n",
      "Epoch 1000 , Error : 1.0\n",
      "Epoch 1000 , Error : 1.0\n",
      "Epoch 1000 , Error : 2.0\n",
      "Epoch 1100 , Error : 1.0\n",
      "Epoch 1100 , Error : 1.0\n",
      "Epoch 1100 , Error : 1.0\n",
      "Epoch 1100 , Error : 2.0\n",
      "Epoch 1200 , Error : 1.0\n",
      "Epoch 1200 , Error : 1.0\n",
      "Epoch 1200 , Error : 1.0\n",
      "Epoch 1200 , Error : 2.0\n",
      "Epoch 1300 , Error : 1.0\n",
      "Epoch 1300 , Error : 1.0\n",
      "Epoch 1300 , Error : 1.0\n",
      "Epoch 1300 , Error : 2.0\n",
      "Epoch 1400 , Error : 1.0\n",
      "Epoch 1400 , Error : 1.0\n",
      "Epoch 1400 , Error : 1.0\n",
      "Epoch 1400 , Error : 2.0\n",
      "Epoch 1500 , Error : 1.0\n",
      "Epoch 1500 , Error : 1.0\n",
      "Epoch 1500 , Error : 1.0\n",
      "Epoch 1500 , Error : 2.0\n",
      "Epoch 1600 , Error : 1.0\n",
      "Epoch 1600 , Error : 1.0\n",
      "Epoch 1600 , Error : 1.0\n",
      "Epoch 1600 , Error : 2.0\n",
      "Epoch 1700 , Error : 1.0\n",
      "Epoch 1700 , Error : 1.0\n",
      "Epoch 1700 , Error : 1.0\n",
      "Epoch 1700 , Error : 2.0\n",
      "Epoch 1800 , Error : 1.0\n",
      "Epoch 1800 , Error : 1.0\n",
      "Epoch 1800 , Error : 1.0\n",
      "Epoch 1800 , Error : 2.0\n",
      "Epoch 1900 , Error : 1.0\n",
      "Epoch 1900 , Error : 1.0\n",
      "Epoch 1900 , Error : 1.0\n",
      "Epoch 1900 , Error : 2.0\n",
      "Epoch 2000 , Error : 1.0\n",
      "Epoch 2000 , Error : 1.0\n",
      "Epoch 2000 , Error : 1.0\n",
      "Epoch 2000 , Error : 2.0\n",
      "Epoch 2100 , Error : 1.0\n",
      "Epoch 2100 , Error : 1.0\n",
      "Epoch 2100 , Error : 1.0\n",
      "Epoch 2100 , Error : 2.0\n",
      "Epoch 2200 , Error : 1.0\n",
      "Epoch 2200 , Error : 1.0\n",
      "Epoch 2200 , Error : 1.0\n",
      "Epoch 2200 , Error : 2.0\n",
      "Epoch 2300 , Error : 1.0\n",
      "Epoch 2300 , Error : 1.0\n",
      "Epoch 2300 , Error : 1.0\n",
      "Epoch 2300 , Error : 2.0\n",
      "Epoch 2400 , Error : 1.0\n",
      "Epoch 2400 , Error : 1.0\n",
      "Epoch 2400 , Error : 1.0\n",
      "Epoch 2400 , Error : 2.0\n",
      "Epoch 2500 , Error : 1.0\n",
      "Epoch 2500 , Error : 1.0\n",
      "Epoch 2500 , Error : 1.0\n",
      "Epoch 2500 , Error : 2.0\n",
      "Epoch 2600 , Error : 1.0\n",
      "Epoch 2600 , Error : 1.0\n",
      "Epoch 2600 , Error : 1.0\n",
      "Epoch 2600 , Error : 2.0\n",
      "Epoch 2700 , Error : 1.0\n",
      "Epoch 2700 , Error : 1.0\n",
      "Epoch 2700 , Error : 1.0\n",
      "Epoch 2700 , Error : 2.0\n",
      "Epoch 2800 , Error : 1.0\n",
      "Epoch 2800 , Error : 1.0\n",
      "Epoch 2800 , Error : 1.0\n",
      "Epoch 2800 , Error : 2.0\n",
      "Epoch 2900 , Error : 1.0\n",
      "Epoch 2900 , Error : 1.0\n",
      "Epoch 2900 , Error : 1.0\n",
      "Epoch 2900 , Error : 2.0\n",
      "Epoch 3000 , Error : 1.0\n",
      "Epoch 3000 , Error : 1.0\n",
      "Epoch 3000 , Error : 1.0\n",
      "Epoch 3000 , Error : 2.0\n",
      "Epoch 3100 , Error : 1.0\n",
      "Epoch 3100 , Error : 1.0\n",
      "Epoch 3100 , Error : 1.0\n",
      "Epoch 3100 , Error : 2.0\n",
      "Epoch 3200 , Error : 1.0\n",
      "Epoch 3200 , Error : 1.0\n",
      "Epoch 3200 , Error : 1.0\n",
      "Epoch 3200 , Error : 2.0\n",
      "Epoch 3300 , Error : 1.0\n",
      "Epoch 3300 , Error : 1.0\n",
      "Epoch 3300 , Error : 1.0\n",
      "Epoch 3300 , Error : 2.0\n",
      "Epoch 3400 , Error : 1.0\n",
      "Epoch 3400 , Error : 1.0\n",
      "Epoch 3400 , Error : 1.0\n",
      "Epoch 3400 , Error : 2.0\n",
      "Epoch 3500 , Error : 1.0\n",
      "Epoch 3500 , Error : 1.0\n",
      "Epoch 3500 , Error : 1.0\n",
      "Epoch 3500 , Error : 2.0\n",
      "Epoch 3600 , Error : 1.0\n",
      "Epoch 3600 , Error : 1.0\n",
      "Epoch 3600 , Error : 1.0\n",
      "Epoch 3600 , Error : 2.0\n",
      "Epoch 3700 , Error : 1.0\n",
      "Epoch 3700 , Error : 1.0\n",
      "Epoch 3700 , Error : 1.0\n",
      "Epoch 3700 , Error : 2.0\n",
      "Epoch 3800 , Error : 1.0\n",
      "Epoch 3800 , Error : 1.0\n",
      "Epoch 3800 , Error : 1.0\n",
      "Epoch 3800 , Error : 2.0\n",
      "Epoch 3900 , Error : 1.0\n",
      "Epoch 3900 , Error : 1.0\n",
      "Epoch 3900 , Error : 1.0\n",
      "Epoch 3900 , Error : 2.0\n",
      "Epoch 4000 , Error : 1.0\n",
      "Epoch 4000 , Error : 1.0\n",
      "Epoch 4000 , Error : 1.0\n",
      "Epoch 4000 , Error : 2.0\n",
      "Epoch 4100 , Error : 1.0\n",
      "Epoch 4100 , Error : 1.0\n",
      "Epoch 4100 , Error : 1.0\n",
      "Epoch 4100 , Error : 2.0\n",
      "Epoch 4200 , Error : 1.0\n",
      "Epoch 4200 , Error : 1.0\n",
      "Epoch 4200 , Error : 1.0\n",
      "Epoch 4200 , Error : 2.0\n",
      "Epoch 4300 , Error : 1.0\n",
      "Epoch 4300 , Error : 1.0\n",
      "Epoch 4300 , Error : 1.0\n",
      "Epoch 4300 , Error : 2.0\n",
      "Epoch 4400 , Error : 1.0\n",
      "Epoch 4400 , Error : 1.0\n",
      "Epoch 4400 , Error : 1.0\n",
      "Epoch 4400 , Error : 2.0\n",
      "Epoch 4500 , Error : 1.0\n",
      "Epoch 4500 , Error : 1.0\n",
      "Epoch 4500 , Error : 1.0\n",
      "Epoch 4500 , Error : 2.0\n",
      "Epoch 4600 , Error : 1.0\n",
      "Epoch 4600 , Error : 1.0\n",
      "Epoch 4600 , Error : 1.0\n",
      "Epoch 4600 , Error : 2.0\n",
      "Epoch 4700 , Error : 1.0\n",
      "Epoch 4700 , Error : 1.0\n",
      "Epoch 4700 , Error : 1.0\n",
      "Epoch 4700 , Error : 2.0\n",
      "Epoch 4800 , Error : 1.0\n",
      "Epoch 4800 , Error : 1.0\n",
      "Epoch 4800 , Error : 1.0\n",
      "Epoch 4800 , Error : 2.0\n",
      "Epoch 4900 , Error : 1.0\n",
      "Epoch 4900 , Error : 1.0\n",
      "Epoch 4900 , Error : 1.0\n",
      "Epoch 4900 , Error : 2.0\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = train(X,y , input_size ,hidden_size , output_size ,learning_rate , epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] -  1\n",
      "[0, 1] -  1\n",
      "[1, 0] -  1\n",
      "[1, 1] -  1\n"
     ]
    }
   ],
   "source": [
    "for sample in X: \n",
    "    print(f'{sample } -  {predict(sample,w1,w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    exps = [m.exp(i) for i in x ] \n",
    "    total = sum(exps)\n",
    "    return [i/total for i in exps ]\n",
    "\n",
    "\n",
    "def softmax_derivative(softmax_output): \n",
    "    n = len(softmax_output) \n",
    "    return [[softmax_output[i] * (1 if i == j else -softmax_output[j]) for j in range(n)] for i in range(n)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w1, w2 \u001b[38;5;241m=\u001b[39m train(X,y , input_size ,hidden_size , output_size ,learning_rate , epochs)\n",
      "Cell \u001b[1;32mIn[45], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, y, input_size, hidden_size, output_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m targets \u001b[38;5;241m=\u001b[39m y[i]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# forward pass \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m hidden_outputs , outputs \u001b[38;5;241m=\u001b[39mforward_propagation(inputs, w1, w2) \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# compute errors  \u001b[39;00m\n\u001b[0;32m     16\u001b[0m total_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  (targets\u001b[38;5;241m-\u001b[39m outputs )\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[1;32mIn[63], line 4\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(inputs, w1, w2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(inputs,w1,w2): \n\u001b[0;32m      3\u001b[0m     hidden_input \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(i \u001b[38;5;241m*\u001b[39m w1[k][j] \u001b[38;5;28;01mfor\u001b[39;00m k, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w1[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[1;32m----> 4\u001b[0m     hidden_output \u001b[38;5;241m=\u001b[39m [softmax(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m hidden_input] \n\u001b[0;32m      6\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(h\u001b[38;5;241m*\u001b[39mw \u001b[38;5;28;01mfor\u001b[39;00m h , w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hidden_output , w2 ))\n\u001b[0;32m      7\u001b[0m     output \u001b[38;5;241m=\u001b[39m softmax(output)\n",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(x): \n\u001b[1;32m----> 2\u001b[0m     exps \u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mexp(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x ] \n\u001b[0;32m      3\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(exps)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [i\u001b[38;5;241m/\u001b[39mtotal \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m exps ]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "w1, w2 = train(X,y , input_size ,hidden_size , output_size ,learning_rate , epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
